{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-répertoire ou un fichier lignesdiff existe déjà.\n"
     ]
    }
   ],
   "source": [
    "mkdir lignesdiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfrod\\lignesdiff\n"
     ]
    }
   ],
   "source": [
    "cd lignesdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd\n",
    "from autograd import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_seed(g,x=0,a=0,b=1,c=0, eps=2**(-26)):\n",
    "    def f(y):\n",
    "        return g(x,y)-c\n",
    "    if not ((f(0)<=c<=f(1))or (f(0)>=c>=f(1))):\n",
    "        return None\n",
    "    while abs(a-b)>eps:\n",
    "        d=(a+b)/2\n",
    "        if f(d)*f(a)>=0:\n",
    "            a=d\n",
    "        else:\n",
    "            b=d\n",
    "    return (a+b)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ici on est dans le cas d'une fonction à une seule variable, puisqu'on étudie en fait f:y g(x,y) avec x fixé donc.L'énoncé proposait de prendre x=0, mais on a besoin d'une forme plus générale pour la question suivante, et on se propose de poser a et b comme arguments définis par défaut à 0 et 1 respectivement (également pour la question suivante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_contour(f,c=0,delta=0.01):\n",
    "    x=0 ; delta_x=delta/3 ; y=find_seed(f)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    if y!=None:\n",
    "        X.append(x)\n",
    "        Y.append(y)#jusqu'à maintenant on initialise \n",
    "    while x<=1:\n",
    "        delta_y=sqrt(delta**2-delta_x**2)#delta_y défini à partir des coordonnées du point précédent pour vérifier l'espacement<=delta\n",
    "        y_=find_seed(f,x,y-delta_y,y+delta_y,c)\n",
    "        y=y_\n",
    "        if y!=None:\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        x+=delta/3\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on a écrit un programme qui parcourt les x en faisant un saut de delta/3 (choix arbitraire), \n",
    "on choisit ensuite les deux points initiaux de la dichotomie de sorte à ce qu'on soit surs que\n",
    "deux antécédents consécutifs de c soient espacés de moins de delta. Cependant, ce programme est\n",
    "efficace dans la limite où on trouve une valeur à chaque saut (dans le cas on renvoie des listes vides).\n",
    "Ce programme est trop particulier, il faudrait plutôt balayer les x dans le cercle de centre (xn,yn) et de\n",
    "rayon delta pour trouver (xn+1,yn+1). Pour cela, on a pensé à mettre au point un programme récursif avec une limite de proximité \n",
    "entre deux points consécutifs pour éviter que le programme ne tourne à l'infini (par exemple sur les bornes \n",
    "d'un intervalle ouvert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2 0. ]\n",
      " [0.  4.6]]\n",
      "[6.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return np.array([x**2,y**2])\n",
    "def J_f(x, y):\n",
    "    return np.c_[autograd.jacobian(f, 0)(x, y), autograd.jacobian(f, 1)(x, y)]\n",
    "print(J_f(3.1, 2.3))\n",
    "\n",
    "print(autograd.jacobian(f)(3.1,2.3))\n",
    "\n",
    "def delta_xy_recursif(f, x, delta = 0.01, delta_rec = 0.01, c = 0, eps = 10**(-5)):\n",
    "    delta_x = delta_rec/2 ; delta_y = sqrt(delta**2-delta_x**2) ; y = find_seed(f, x, y-delt_y, y+delta_y, c)\n",
    "    if delta_x > eps:\n",
    "        if y != None:\n",
    "            return (delta_x, y)\n",
    "        else : \n",
    "            return delta_xy_recursif(f, x, delta, delta_x, c, eps)\n",
    "    else : \n",
    "        return None \n",
    "\n",
    "def simple_contour_rec(f, c = 0, delta = 0.01, eps = 10**(-5)):\n",
    "    x=0 ; y= find_seed(f, x, 0, 1, c) ; i = 0\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    if y != None : \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    while x <= 1 and i<=10000:\n",
    "        if delta_xy_recursif != None : \n",
    "            (delta_x, y) = delta_xy_recursif\n",
    "            x += delta_x\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            i += 1\n",
    "        else :\n",
    "            return None\n",
    "    return (X,Y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comme expliqué précédemment, dans le programme ci-dessus on programme de manière récursive: à chaque fois qu'on avance sur l'axe des x, \n",
    "on tente de trouver (x_,y_) proche de moins de delta de nos (x,y) tel que f(x,y)=c. Pour cela, on cherche un y qui fonctionne en delta/2, \n",
    "puis en delta/4, puis en delta/8, puis en delta/16, etc... en s'arrêtant à delta_x<eps. On avance donc avec un pas qui dépend de x, donc on peut potentiellement ne jamais s'arrêter, d'où le compteur i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(L):\n",
    "    s=0 \n",
    "    for i in range(len(L)):\n",
    "        s+=L[i]**2\n",
    "    return sqrt(s)\n",
    "\n",
    "def trouv_reg(f, x, y, x_, y_, delta):#x_=xn, x=xn-1\n",
    "    gradi=autograd.grad(f)(x_,y_)\n",
    "    orth=(-gradi[1], gradi[0])/norm(orth)*delta\n",
    "    if norm((x_,y_)-orth-(x,y))>norm((x_,y_)+orth-(x,y)):\n",
    "        return (x_,y_)-orth\n",
    "    return (x_,y_)+orth\n",
    "\n",
    "def newton(f,x,y,x_,y_,delta,c,n):\n",
    "    (p,q)=(x_,y_)\n",
    "    def g(x_,y_):\n",
    "        return np.array([f(x_,y_)-c, norm((x_-x,y_-y))**2-delta**2])\n",
    "    J=np.c_[autograd.Jacobian(g,0)(p,q), autograd.Jacobian(g,1)(p,q)]\n",
    "    for i in range(n):\n",
    "        if np.det(J)==0:\n",
    "            raise ValueError(\"méthode de Newton non applicable\")\n",
    "        translat=-np.inv(J)*g(f,x,y,p,q,delta,c)\n",
    "        (p,q)=(p+translat[0], q+translat[1])\n",
    "        J=np.c_[autograd.Jacobian(g,0)(p,q), autograd.Jacobian(g,1)(p,q)]\n",
    "    return (p,q)\n",
    "\n",
    "def contour_simple_final(f, c, delta, eps, n):\n",
    "    x_=0 ; y_=find_seed(f)\n",
    "    X=[x_]\n",
    "    Y=[y_]\n",
    "    gradi=autograd.grad(f)(x_,y_)\n",
    "    orth=(-gradi[1], gradi[0])/norm(orth)*delta\n",
    "    (x,y)=(x_,y_)\n",
    "    if orth[0]<0:\n",
    "        (x_,y_)=(x_-orth[0],y_-orth[1])\n",
    "    else:\n",
    "        (x_,y_)=(x_+orth[0],y_+orth[1])\n",
    "    (x_,y_)=newton(f,x,y,x_,y_,delta,c,n)\n",
    "    X.append(x_)\n",
    "    Y.append(y_)\n",
    "    \n",
    "    while (x<=1 & y<=1):\n",
    "        (x_,y_)=trouv_reg(f,x,y,x_,y_,delta)\n",
    "        (x,y)=(X[-1],Y[-1])\n",
    "        (x_,y_)=newton(f,x,y,x_,y_,delta,c,n)\n",
    "        X.append(x_)\n",
    "        Y.append(y_)\n",
    "    return (X,Y)    \n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
